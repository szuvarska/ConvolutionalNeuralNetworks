{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-26T21:36:51.686251Z",
     "start_time": "2025-04-26T21:36:48.488400Z"
    }
   },
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"./../../src\")\n",
    "\n",
    "\n",
    "from Dataset import SpeechCommandsDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "from torch import optim\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "import torchaudio.transforms as T\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T21:45:17.063788Z",
     "start_time": "2025-04-26T21:45:17.047540Z"
    }
   },
   "cell_type": "code",
   "source": "torch.cuda.empty_cache()",
   "id": "a15559ac93b77868",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T21:36:52.012321Z",
     "start_time": "2025-04-26T21:36:51.690047Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataset = SpeechCommandsDataset(\"./../../data/train\", mode=\"modified\")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=6)"
   ],
   "id": "fd30b04daa87cc03",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T21:36:52.017561Z",
     "start_time": "2025-04-26T21:36:52.013479Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torchaudio.transforms as T\n",
    "\n",
    "mel_transform = T.MelSpectrogram(\n",
    "    sample_rate=16000,\n",
    "    n_mels=64\n",
    ")\n",
    "\n",
    "# Optional: Log-compression (better for neural nets)\n",
    "log_mel_transform = torch.nn.Sequential(\n",
    "    mel_transform,\n",
    "    T.AmplitudeToDB()\n",
    ")\n"
   ],
   "id": "82a8ca847e8dfddf",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T21:39:03.372580Z",
     "start_time": "2025-04-26T21:39:03.365837Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SpeechCommandTransformer(nn.Module):\n",
    "    def __init__(self, num_classes: int, n_mels: int = 64, embed_dim: int = 32, num_layers: int = 4, num_heads: int = 4, device: torch.device = None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.device = device if device else torch.device(\"cpu\")\n",
    "        \n",
    "        self.feature_extractor = T.MelSpectrogram(\n",
    "            sample_rate=16000,\n",
    "            n_fft=400,\n",
    "            hop_length=160,\n",
    "            n_mels=n_mels\n",
    "        )\n",
    "        \n",
    "        self.db = torchaudio.transforms.AmplitudeToDB()\n",
    "\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.projection = nn.Linear(n_mels, embed_dim)  # Project mel bins to embed_dim\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerEncoderLayer(\n",
    "                d_model=embed_dim,\n",
    "                nhead=num_heads,\n",
    "                dim_feedforward=512,\n",
    "                dropout=0.1,\n",
    "                activation=\"gelu\",\n",
    "                batch_first=True\n",
    "            )\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim)) # Learnable [CLS] token\n",
    "\n",
    "        self.classifier = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, waveforms: torch.Tensor):\n",
    "        \"\"\"\n",
    "        waveforms: List[Tensor] or Tensor shape (batch_size, samples)\n",
    "        \"\"\"\n",
    "\n",
    "        if isinstance(waveforms, list):\n",
    "            waveforms = torch.nn.utils.rnn.pad_sequence(waveforms, batch_first=True)\n",
    "\n",
    "        features = self.feature_extractor(waveforms)  # (batch_size, n_mels, time)\n",
    "        features = self.db(features)  # (batch_size, n_mels, time)\n",
    "        features = self.cnn(features.unsqueeze(1))  # (batch_size, 64, n_mels, time)\n",
    "\n",
    "        # Flatten the frequency and time dims into a sequence\n",
    "        batch_size, channels, freq, time = features.shape\n",
    "        features = features.permute(0, 2, 3, 1)  # (batch_size, freq, time, channels)\n",
    "        features = features.reshape(batch_size, freq * time, channels)  # (batch_size, seq_len, channels)\n",
    "        \n",
    "        x = self.projection(features)  # (batch_size, seq_len, embed_dim)\n",
    "\n",
    "\n",
    "        # Add CLS token\n",
    "        batch_size = x.size(0)\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)  # (batch_size, 1, embed_dim)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)  # (batch_size, 1 + time, embed_dim)\n",
    "\n",
    "        from torch.utils.checkpoint import checkpoint_sequential\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "            print(f\"Layer output shape: {x.shape}\")\n",
    "\n",
    "        cls_output = x[:, 0]  # take output of CLS token\n",
    "\n",
    "        logits = self.classifier(cls_output)\n",
    "\n",
    "        return logits"
   ],
   "id": "7369f8c3c3bda4fa",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T21:39:04.181885Z",
     "start_time": "2025-04-26T21:39:03.934487Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = SpeechCommandTransformer(num_classes=len(train_dataset.class_to_idx), device=device).to(device)\n",
    "\n",
    "# Example batch\n",
    "for waveforms, labels in train_loader:\n",
    "    waveforms = waveforms.squeeze(1).to(device)  # optional\n",
    "    logits = model(waveforms)  # logits shape (batch_size, num_classes)\n",
    "    print(logits.shape)\n",
    "    break"
   ],
   "id": "801d16774530ba55",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer output shape: torch.Size([8, 6465, 32])\n",
      "Layer output shape: torch.Size([8, 6465, 32])\n",
      "Layer output shape: torch.Size([8, 6465, 32])\n",
      "Layer output shape: torch.Size([8, 6465, 32])\n",
      "torch.Size([8, 12])\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T21:41:40.048485Z",
     "start_time": "2025-04-26T21:41:40.045416Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.optim as optim\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "num_epochs = 10"
   ],
   "id": "9defc14371b9d473",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T21:42:55.421564Z",
     "start_time": "2025-04-26T21:41:40.568131Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "scaler = torch.amp.GradScaler()\n",
    "for epoch in range(num_epochs):\n",
    "    # Memory usage before the epoch\n",
    "    print(\"Memory before epoch:\", torch.cuda.memory_allocated(device))\n",
    "\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "    for waveforms, labels in pbar:\n",
    "        waveforms = waveforms.squeeze(1).to(device)  # (batch_size, samples)\n",
    "        waveforms.requires_grad_()\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.amp.autocast(device_type='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "            outputs = model(waveforms)\n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "\n",
    "        running_loss += loss.item() * waveforms.size(0)\n",
    "\n",
    "        # Compute accuracy\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "        pbar.set_postfix(loss=running_loss / total, acc=100. * correct / total)\n",
    "\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Memory usage after each batch\n",
    "    print(\"Memory after batch:\", torch.cuda.memory_allocated(device))\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}, Accuracy: {100.*correct/total:.2f}%\")\n",
    "    \n",
    "    # Memory usage after the epoch\n",
    "    print(\"Memory after epoch:\", torch.cuda.memory_allocated(device))\n",
    "    \n",
    "# After training\n",
    "print(\"Total memory allocated:\", torch.cuda.memory_allocated(device))\n",
    "print(\"Max memory allocated:\", torch.cuda.max_memory_allocated(device))\n",
    "\n",
    "# Optionally, reset max memory stats\n",
    "torch.cuda.reset_max_memory_allocated(device)"
   ],
   "id": "bbd1db577b10f299",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory before epoch: 1363348480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 0/6583 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer output shape: torch.Size([8, 6465, 32])\n",
      "Layer output shape: torch.Size([8, 6465, 32])\n",
      "Layer output shape: torch.Size([8, 6465, 32])\n",
      "Layer output shape: torch.Size([8, 6465, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 0/6583 [01:14<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[16], line 27\u001B[0m\n\u001B[1;32m     24\u001B[0m     loss \u001B[38;5;241m=\u001B[39m criterion(outputs, labels)\n\u001B[1;32m     26\u001B[0m scaler\u001B[38;5;241m.\u001B[39mscale(loss)\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[0;32m---> 27\u001B[0m \u001B[43mscaler\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     28\u001B[0m scaler\u001B[38;5;241m.\u001B[39mupdate()\n\u001B[1;32m     31\u001B[0m running_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem() \u001B[38;5;241m*\u001B[39m waveforms\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m)\n",
      "File \u001B[0;32m/media/marta/Dane/Sem8/ConvolutionalNeuralNeutworks/.venv/lib/python3.10/site-packages/torch/amp/grad_scaler.py:457\u001B[0m, in \u001B[0;36mGradScaler.step\u001B[0;34m(self, optimizer, *args, **kwargs)\u001B[0m\n\u001B[1;32m    451\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39munscale_(optimizer)\n\u001B[1;32m    453\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m (\n\u001B[1;32m    454\u001B[0m     \u001B[38;5;28mlen\u001B[39m(optimizer_state[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfound_inf_per_device\u001B[39m\u001B[38;5;124m\"\u001B[39m]) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m    455\u001B[0m ), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo inf checks were recorded for this optimizer.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m--> 457\u001B[0m retval \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_maybe_opt_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer_state\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    459\u001B[0m optimizer_state[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstage\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m OptState\u001B[38;5;241m.\u001B[39mSTEPPED\n\u001B[1;32m    461\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m retval\n",
      "File \u001B[0;32m/media/marta/Dane/Sem8/ConvolutionalNeuralNeutworks/.venv/lib/python3.10/site-packages/torch/amp/grad_scaler.py:351\u001B[0m, in \u001B[0;36mGradScaler._maybe_opt_step\u001B[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001B[0m\n\u001B[1;32m    343\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m_maybe_opt_step\u001B[39m(\n\u001B[1;32m    344\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    345\u001B[0m     optimizer: torch\u001B[38;5;241m.\u001B[39moptim\u001B[38;5;241m.\u001B[39mOptimizer,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    348\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[1;32m    349\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Optional[\u001B[38;5;28mfloat\u001B[39m]:\n\u001B[1;32m    350\u001B[0m     retval: Optional[\u001B[38;5;28mfloat\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 351\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;43msum\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43moptimizer_state\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfound_inf_per_device\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalues\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[1;32m    352\u001B[0m         retval \u001B[38;5;241m=\u001B[39m optimizer\u001B[38;5;241m.\u001B[39mstep(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    353\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m retval\n",
      "File \u001B[0;32m/media/marta/Dane/Sem8/ConvolutionalNeuralNeutworks/.venv/lib/python3.10/site-packages/torch/amp/grad_scaler.py:351\u001B[0m, in \u001B[0;36m<genexpr>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    343\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m_maybe_opt_step\u001B[39m(\n\u001B[1;32m    344\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    345\u001B[0m     optimizer: torch\u001B[38;5;241m.\u001B[39moptim\u001B[38;5;241m.\u001B[39mOptimizer,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    348\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[1;32m    349\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Optional[\u001B[38;5;28mfloat\u001B[39m]:\n\u001B[1;32m    350\u001B[0m     retval: Optional[\u001B[38;5;28mfloat\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 351\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28msum\u001B[39m(\u001B[43mv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m v \u001B[38;5;129;01min\u001B[39;00m optimizer_state[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfound_inf_per_device\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mvalues()):\n\u001B[1;32m    352\u001B[0m         retval \u001B[38;5;241m=\u001B[39m optimizer\u001B[38;5;241m.\u001B[39mstep(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    353\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m retval\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T21:48:08.987245Z",
     "start_time": "2025-04-26T21:48:08.981915Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "from torch.utils.data import DataLoader\n",
    "from Dataset import SpeechCommandsDataset\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes: int, n_mels: int = 64):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "\n",
    "        # Mel spectrogram transform\n",
    "        self.mel_transform = T.MelSpectrogram(sample_rate=16000, n_mels=n_mels)\n",
    "        self.db_transform = T.AmplitudeToDB()\n",
    "\n",
    "        # Simple CNN model\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * n_mels * 8, 128)  # Adjust input size based on output of conv layers\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, waveforms: torch.Tensor):\n",
    "        # Apply mel spectrogram transform and convert to decibels\n",
    "        mel_spec = self.mel_transform(waveforms)  # (batch_size, n_mels, time)\n",
    "        mel_spec = self.db_transform(mel_spec)  # Convert to dB scale\n",
    "\n",
    "        # Add a channel dimension for Conv2d\n",
    "        mel_spec = mel_spec.unsqueeze(1)  # (batch_size, 1, n_mels, time)\n",
    "\n",
    "        # Convolutional layers\n",
    "        x = self.pool(torch.relu(self.conv1(mel_spec)))  # (batch_size, 32, n_mels, time/2)\n",
    "        x = self.pool(torch.relu(self.conv2(x)))  # (batch_size, 64, n_mels, time/4)\n",
    "\n",
    "        # Flatten and feed through fully connected layers\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = torch.relu(self.fc1(x))  # (batch_size, 128)\n",
    "        x = self.fc2(x)  # (batch_size, num_classes)\n",
    "\n",
    "        return x\n"
   ],
   "id": "1effa0535d5c4ef6",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T21:48:10.030520Z",
     "start_time": "2025-04-26T21:48:09.555057Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the dataset\n",
    "train_dataset = SpeechCommandsDataset(\"./../../data/train\", mode=\"modified\")\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "\n",
    "# Create the model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = SimpleCNN(num_classes=len(train_dataset.class_to_idx)).to(device)\n"
   ],
   "id": "c79f10fe1082ec09",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T21:48:10.906770Z",
     "start_time": "2025-04-26T21:48:10.449478Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    for waveforms, labels in pbar:\n",
    "        waveforms = waveforms.squeeze(1).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(waveforms)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update running loss and accuracy\n",
    "        running_loss += loss.item() * waveforms.size(0)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "        pbar.set_postfix(loss=running_loss / total, acc=100. * correct / total)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}, Accuracy: {100.*correct/total:.2f}%\")\n"
   ],
   "id": "746eac80506b95c7",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 0/1646 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (32x20480 and 32768x128)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[23], line 26\u001B[0m\n\u001B[1;32m     23\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m     25\u001B[0m \u001B[38;5;66;03m# Forward pass\u001B[39;00m\n\u001B[0;32m---> 26\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mwaveforms\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     27\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(outputs, labels)\n\u001B[1;32m     29\u001B[0m \u001B[38;5;66;03m# Backward pass\u001B[39;00m\n",
      "File \u001B[0;32m/media/marta/Dane/Sem8/ConvolutionalNeuralNeutworks/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/media/marta/Dane/Sem8/ConvolutionalNeuralNeutworks/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "Cell \u001B[0;32mIn[21], line 37\u001B[0m, in \u001B[0;36mSimpleCNN.forward\u001B[0;34m(self, waveforms)\u001B[0m\n\u001B[1;32m     35\u001B[0m \u001B[38;5;66;03m# Flatten and feed through fully connected layers\u001B[39;00m\n\u001B[1;32m     36\u001B[0m x \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mview(x\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m), \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)  \u001B[38;5;66;03m# Flatten\u001B[39;00m\n\u001B[0;32m---> 37\u001B[0m x \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mrelu(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfc1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m)  \u001B[38;5;66;03m# (batch_size, 128)\u001B[39;00m\n\u001B[1;32m     38\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfc2(x)  \u001B[38;5;66;03m# (batch_size, num_classes)\u001B[39;00m\n\u001B[1;32m     40\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "File \u001B[0;32m/media/marta/Dane/Sem8/ConvolutionalNeuralNeutworks/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/media/marta/Dane/Sem8/ConvolutionalNeuralNeutworks/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m/media/marta/Dane/Sem8/ConvolutionalNeuralNeutworks/.venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:125\u001B[0m, in \u001B[0;36mLinear.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    124\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 125\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: mat1 and mat2 shapes cannot be multiplied (32x20480 and 32768x128)"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ca4b75947f465b37"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
