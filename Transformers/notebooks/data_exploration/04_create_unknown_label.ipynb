{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-26T19:15:38.261274Z",
     "start_time": "2025-04-26T19:15:34.432829Z"
    }
   },
   "source": [
    "from pathlib import Path\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T19:19:34.444694Z",
     "start_time": "2025-04-26T19:19:34.439263Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SpeechCommandsDataset(Dataset):\n",
    "    def __init__(self, root_dir: str, transform=None, mode: str = \"original\",\n",
    "        commands: Optional[List[str]] = None,  # list of command labels\n",
    "    ):\n",
    "        assert mode in {\"original\", \"modified\"}, \"mode must be 'original' or 'modified'\"\n",
    "        \n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.transform = transform\n",
    "        self.mode = mode\n",
    "        \n",
    "        # Set the known commands if provided, otherwise use default\n",
    "        if commands is None:\n",
    "            self.commands = [\n",
    "                \"yes\", \"no\", \"up\", \"down\", \"left\", \"right\", \"on\", \"off\", \"stop\", \"go\"\n",
    "            ]\n",
    "        else:\n",
    "            self.commands = commands\n",
    "\n",
    "        # Build samples\n",
    "        self.samples = []\n",
    "        all_labels = sorted({p.name for p in self.root_dir.iterdir() if p.is_dir()})\n",
    "        \n",
    "        if self.mode == \"original\":\n",
    "            self.labels = all_labels\n",
    "        else:  # modified\n",
    "            self.labels = sorted(self.commands + [\"unknown\"])\n",
    "        \n",
    "        self.label_to_index = {label: idx for idx, label in enumerate(self.labels)}\n",
    "\n",
    "        for label in all_labels:\n",
    "            label_dir = self.root_dir / label\n",
    "            for wav_file in label_dir.glob(\"**/*.wav\"):\n",
    "                if self.mode == \"original\":\n",
    "                    target_label = label\n",
    "                else:\n",
    "                    target_label = label if label in self.commands else \"unknown\"\n",
    "                self.samples.append((wav_file, self.label_to_index[target_label]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx) -> Tuple[torch.Tensor, int]:\n",
    "        audio_path, label = self.samples[idx]\n",
    "        waveform, sample_rate = torchaudio.load(audio_path)\n",
    "\n",
    "        if self.transform:\n",
    "            waveform = self.transform(waveform)\n",
    "\n",
    "        return waveform, label"
   ],
   "id": "892c6d3762f7c760",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T19:19:35.291245Z",
     "start_time": "2025-04-26T19:19:35.289159Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def collate_fn(batch):\n",
    "    waveforms, labels = zip(*batch)\n",
    "    return list(waveforms), torch.tensor(labels)"
   ],
   "id": "a6e5c34ceb023ad0",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T19:23:35.559421Z",
     "start_time": "2025-04-26T19:23:35.113964Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataset = SpeechCommandsDataset(\"./../../data/train\", mode=\"modified\")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=40, shuffle=True, collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "for batch in train_loader:\n",
    "    waveforms, labels = batch\n",
    "    for i, waveform in enumerate(waveforms):\n",
    "        print(f\"Waveform {i} shape: {waveform.shape}\")\n",
    "    print(labels)\n",
    "    break"
   ],
   "id": "e6336fbd25253429",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waveform 0 shape: torch.Size([1, 16000])\n",
      "Waveform 1 shape: torch.Size([1, 16000])\n",
      "Waveform 2 shape: torch.Size([1, 16000])\n",
      "Waveform 3 shape: torch.Size([1, 16000])\n",
      "Waveform 4 shape: torch.Size([1, 9558])\n",
      "Waveform 5 shape: torch.Size([1, 16000])\n",
      "Waveform 6 shape: torch.Size([1, 16000])\n",
      "Waveform 7 shape: torch.Size([1, 16000])\n",
      "Waveform 8 shape: torch.Size([1, 16000])\n",
      "Waveform 9 shape: torch.Size([1, 16000])\n",
      "Waveform 10 shape: torch.Size([1, 16000])\n",
      "Waveform 11 shape: torch.Size([1, 16000])\n",
      "Waveform 12 shape: torch.Size([1, 16000])\n",
      "Waveform 13 shape: torch.Size([1, 12971])\n",
      "Waveform 14 shape: torch.Size([1, 16000])\n",
      "Waveform 15 shape: torch.Size([1, 16000])\n",
      "Waveform 16 shape: torch.Size([1, 16000])\n",
      "Waveform 17 shape: torch.Size([1, 16000])\n",
      "Waveform 18 shape: torch.Size([1, 16000])\n",
      "Waveform 19 shape: torch.Size([1, 16000])\n",
      "Waveform 20 shape: torch.Size([1, 16000])\n",
      "Waveform 21 shape: torch.Size([1, 16000])\n",
      "Waveform 22 shape: torch.Size([1, 16000])\n",
      "Waveform 23 shape: torch.Size([1, 9558])\n",
      "Waveform 24 shape: torch.Size([1, 16000])\n",
      "Waveform 25 shape: torch.Size([1, 16000])\n",
      "Waveform 26 shape: torch.Size([1, 16000])\n",
      "Waveform 27 shape: torch.Size([1, 16000])\n",
      "Waveform 28 shape: torch.Size([1, 16000])\n",
      "Waveform 29 shape: torch.Size([1, 16000])\n",
      "Waveform 30 shape: torch.Size([1, 16000])\n",
      "Waveform 31 shape: torch.Size([1, 16000])\n",
      "Waveform 32 shape: torch.Size([1, 16000])\n",
      "Waveform 33 shape: torch.Size([1, 16000])\n",
      "Waveform 34 shape: torch.Size([1, 12971])\n",
      "Waveform 35 shape: torch.Size([1, 16000])\n",
      "Waveform 36 shape: torch.Size([1, 16000])\n",
      "Waveform 37 shape: torch.Size([1, 16000])\n",
      "Waveform 38 shape: torch.Size([1, 16000])\n",
      "Waveform 39 shape: torch.Size([1, 15702])\n",
      "tensor([ 8,  0,  7,  8,  8,  8,  8,  8,  8,  8,  6,  8,  1,  1, 10,  8,  8,  8,\n",
      "         6,  8,  8,  9,  8,  8, 10,  4,  4,  3,  8,  8,  8,  8,  8,  8,  8,  5,\n",
      "         8,  8,  8,  1])\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T19:23:42.141756Z",
     "start_time": "2025-04-26T19:23:42.128008Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import Counter\n",
    "\n",
    "label_counts = Counter()\n",
    "\n",
    "for _, label in train_dataset.samples:\n",
    "    label_counts[label] += 1\n",
    "\n",
    "index_to_label = {idx: label for label, idx in train_dataset.label_to_index.items()}\n",
    "\n",
    "for label_idx, count in label_counts.items():\n",
    "    print(f\"{index_to_label[label_idx]}: {count} samples\")\n"
   ],
   "id": "2aeae79051de25df",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unknown: 34123 samples\n",
      "down: 1842 samples\n",
      "go: 1861 samples\n",
      "left: 1839 samples\n",
      "no: 1853 samples\n",
      "off: 1839 samples\n",
      "on: 1864 samples\n",
      "right: 1852 samples\n",
      "stop: 1885 samples\n",
      "up: 1843 samples\n",
      "yes: 1860 samples\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T19:40:41.462734Z",
     "start_time": "2025-04-26T19:40:38.126576Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.Dataset import SpeechCommandsDataset\n",
    "from collections import Counter\n",
    "train_dataset = SpeechCommandsDataset(\"./../../data/train\", mode=\"modified\")\n",
    "label_counts = Counter()\n",
    "\n",
    "for _, label in train_dataset.samples:\n",
    "    label_counts[label] += 1\n",
    "\n",
    "index_to_label = {idx: label for label, idx in train_dataset.label_to_index.items()}\n",
    "\n",
    "for label_idx, count in label_counts.items():\n",
    "    print(f\"{index_to_label[label_idx]}: {count} samples\")"
   ],
   "id": "52b64731d25c7241",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unknown: 32550 samples\n",
      "down: 1842 samples\n",
      "go: 1861 samples\n",
      "left: 1839 samples\n",
      "no: 1853 samples\n",
      "off: 1839 samples\n",
      "on: 1864 samples\n",
      "right: 1852 samples\n",
      "silence: 1573 samples\n",
      "stop: 1885 samples\n",
      "up: 1843 samples\n",
      "yes: 1860 samples\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "202e6b84a313da03"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
