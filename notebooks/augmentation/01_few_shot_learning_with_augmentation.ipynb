{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-01T08:12:15.041286Z",
     "start_time": "2025-04-01T08:12:05.053333Z"
    }
   },
   "source": [
    "from src.few_shot_learning import load_cinic10, calculate_accuracy, plot_confusion_matrix, FewShotConvNeXt, train_few_shot_convnext, set_seed\n",
    "from src.plots_functions import multiple_runs_with_uncertainty_band"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T08:12:15.052215Z",
     "start_time": "2025-04-01T08:12:15.042512Z"
    }
   },
   "cell_type": "code",
   "source": "set_seed(213)",
   "id": "58aca77eb4f69923",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T08:12:15.397796Z",
     "start_time": "2025-04-01T08:12:15.053028Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_dir = \"../../data\"\n",
    "dataset_name = \"augmented_cinic10_flip\"\n",
    "train_dataloader = load_cinic10(data_dir, few_shot_per_class=100, dataset_name=dataset_name, split=\"none\")\n",
    "test_dataloader = load_cinic10(data_dir, few_shot_per_class=100, split=\"test\")"
   ],
   "id": "6b88e96e17fd410e",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T08:13:34.275700Z",
     "start_time": "2025-04-01T08:13:33.110092Z"
    }
   },
   "cell_type": "code",
   "source": [
    "metrics_list = []\n",
    "for _ in range(5):\n",
    "    model = FewShotConvNeXt()\n",
    "    metrics, time = train_few_shot_convnext(model, train_dataloader, test_dataloader, epochs=10, lr=0.0001, optimizer='adam', scheduling=True)\n",
    "    metrics_list.append(metrics)"
   ],
   "id": "bc43e2596afbca79",
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 3.93 GiB of which 26.94 MiB is free. Process 42880 has 2.87 GiB memory in use. Including non-PyTorch memory, this process has 746.00 MiB memory in use. Of the allocated memory 651.70 MiB is allocated by PyTorch, and 22.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 4\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m5\u001B[39m):\n\u001B[1;32m      3\u001B[0m     model \u001B[38;5;241m=\u001B[39m FewShotConvNeXt()\n\u001B[0;32m----> 4\u001B[0m     metrics, time \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_few_shot_convnext\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.0001\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43madam\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscheduling\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m      5\u001B[0m     metrics_list\u001B[38;5;241m.\u001B[39mappend(metrics)\n",
      "File \u001B[0;32m/media/marta/Dane/Sem8/ConvolutionalNeuralNeutworks/src/few_shot_learning.py:157\u001B[0m, in \u001B[0;36mtrain_few_shot_convnext\u001B[0;34m(model, train_dataloader, test_dataloader, epochs, lr, optimizer, scheduling, silent, weight_decay)\u001B[0m\n\u001B[1;32m    154\u001B[0m         scheduler \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39moptim\u001B[38;5;241m.\u001B[39mlr_scheduler\u001B[38;5;241m.\u001B[39mCosineAnnealingLR(optimizer, T_max\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m)\n\u001B[1;32m    156\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(epochs):\n\u001B[0;32m--> 157\u001B[0m     train_loss, train_acc \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_step\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    158\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdata_loader\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    159\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    160\u001B[0m \u001B[43m        \u001B[49m\u001B[43mloss_fn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    161\u001B[0m \u001B[43m        \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    162\u001B[0m \u001B[43m        \u001B[49m\u001B[43maccuracy_fn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maccuracy_fn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    163\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    164\u001B[0m \u001B[43m        \u001B[49m\u001B[43msilent\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msilent\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    165\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    166\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m scheduling:\n\u001B[1;32m    167\u001B[0m         scheduler\u001B[38;5;241m.\u001B[39mstep()  \u001B[38;5;66;03m# Update LR\u001B[39;00m\n",
      "File \u001B[0;32m/media/marta/Dane/Sem8/ConvolutionalNeuralNeutworks/src/helper_functions.py:37\u001B[0m, in \u001B[0;36mtrain_step\u001B[0;34m(model, data_loader, loss_fn, optimizer, accuracy_fn, device, silent)\u001B[0m\n\u001B[1;32m     34\u001B[0m     loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[1;32m     36\u001B[0m     \u001B[38;5;66;03m# 5. Optimizer step\u001B[39;00m\n\u001B[0;32m---> 37\u001B[0m     \u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     39\u001B[0m \u001B[38;5;66;03m# Calculate loss and accuracy per epoch and print out what's happening\u001B[39;00m\n\u001B[1;32m     40\u001B[0m train_loss \u001B[38;5;241m/\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(data_loader)\n",
      "File \u001B[0;32m/media/marta/Dane/Sem8/ConvolutionalNeuralNeutworks/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:140\u001B[0m, in \u001B[0;36mLRScheduler.__init__.<locals>.patch_track_step_called.<locals>.wrap_step.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    138\u001B[0m opt \u001B[38;5;241m=\u001B[39m opt_ref()\n\u001B[1;32m    139\u001B[0m opt\u001B[38;5;241m.\u001B[39m_opt_called \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m  \u001B[38;5;66;03m# type: ignore[union-attr]\u001B[39;00m\n\u001B[0;32m--> 140\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__get__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mopt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mopt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;18;43m__class__\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/media/marta/Dane/Sem8/ConvolutionalNeuralNeutworks/.venv/lib/python3.10/site-packages/torch/optim/optimizer.py:493\u001B[0m, in \u001B[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    488\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    489\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    490\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresult\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    491\u001B[0m             )\n\u001B[0;32m--> 493\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    494\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_optimizer_step_code()\n\u001B[1;32m    496\u001B[0m \u001B[38;5;66;03m# call optimizer step post hooks\u001B[39;00m\n",
      "File \u001B[0;32m/media/marta/Dane/Sem8/ConvolutionalNeuralNeutworks/.venv/lib/python3.10/site-packages/torch/optim/optimizer.py:91\u001B[0m, in \u001B[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     89\u001B[0m     torch\u001B[38;5;241m.\u001B[39mset_grad_enabled(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdefaults[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdifferentiable\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[1;32m     90\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_dynamo\u001B[38;5;241m.\u001B[39mgraph_break()\n\u001B[0;32m---> 91\u001B[0m     ret \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     92\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     93\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_dynamo\u001B[38;5;241m.\u001B[39mgraph_break()\n",
      "File \u001B[0;32m/media/marta/Dane/Sem8/ConvolutionalNeuralNeutworks/.venv/lib/python3.10/site-packages/torch/optim/adam.py:234\u001B[0m, in \u001B[0;36mAdam.step\u001B[0;34m(self, closure)\u001B[0m\n\u001B[1;32m    231\u001B[0m     state_steps: List[Tensor] \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m    232\u001B[0m     beta1, beta2 \u001B[38;5;241m=\u001B[39m group[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbetas\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m--> 234\u001B[0m     has_complex \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_init_group\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    235\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    236\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparams_with_grad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    237\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    238\u001B[0m \u001B[43m        \u001B[49m\u001B[43mexp_avgs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    239\u001B[0m \u001B[43m        \u001B[49m\u001B[43mexp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    240\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_exp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    241\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    242\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    244\u001B[0m     adam(\n\u001B[1;32m    245\u001B[0m         params_with_grad,\n\u001B[1;32m    246\u001B[0m         grads,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    264\u001B[0m         found_inf\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfound_inf\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[1;32m    265\u001B[0m     )\n\u001B[1;32m    267\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m loss\n",
      "File \u001B[0;32m/media/marta/Dane/Sem8/ConvolutionalNeuralNeutworks/.venv/lib/python3.10/site-packages/torch/optim/adam.py:178\u001B[0m, in \u001B[0;36mAdam._init_group\u001B[0;34m(self, group, params_with_grad, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps)\u001B[0m\n\u001B[1;32m    174\u001B[0m state[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mexp_avg\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mzeros_like(\n\u001B[1;32m    175\u001B[0m     p, memory_format\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mpreserve_format\n\u001B[1;32m    176\u001B[0m )\n\u001B[1;32m    177\u001B[0m \u001B[38;5;66;03m# Exponential moving average of squared gradient values\u001B[39;00m\n\u001B[0;32m--> 178\u001B[0m state[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mexp_avg_sq\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mzeros_like\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    179\u001B[0m \u001B[43m    \u001B[49m\u001B[43mp\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmemory_format\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpreserve_format\u001B[49m\n\u001B[1;32m    180\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    181\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m group[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mamsgrad\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n\u001B[1;32m    182\u001B[0m     \u001B[38;5;66;03m# Maintains max of all exp. moving avg. of sq. grad. values\u001B[39;00m\n\u001B[1;32m    183\u001B[0m     state[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmax_exp_avg_sq\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mzeros_like(\n\u001B[1;32m    184\u001B[0m         p, memory_format\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mpreserve_format\n\u001B[1;32m    185\u001B[0m     )\n",
      "\u001B[0;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 3.93 GiB of which 26.94 MiB is free. Process 42880 has 2.87 GiB memory in use. Including non-PyTorch memory, this process has 746.00 MiB memory in use. Of the allocated memory 651.70 MiB is allocated by PyTorch, and 22.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T08:12:18.879497Z",
     "start_time": "2025-04-01T08:12:18.879413Z"
    }
   },
   "cell_type": "code",
   "source": [
    "experiment_name = \"FewShotConvNeXt with flip augmentation\"\n",
    "multiple_runs_with_uncertainty_band(metrics_list, f\"Accuracy for {experiment_name}\", f\"Loss for {experiment_name}\")"
   ],
   "id": "f4ab8cd893772c43",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data_dir = \"../../data\"\n",
    "dataset_name = \"augmented_cinic10_rotation\"\n",
    "train_dataloader = load_cinic10(data_dir, few_shot_per_class=100, dataset_name=dataset_name, split=\"none\")\n",
    "test_dataloader = load_cinic10(data_dir, few_shot_per_class=100, split=\"test\")"
   ],
   "id": "dd42f7315a275de9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "metrics_list = []\n",
    "for _ in range(5):\n",
    "    model = FewShotConvNeXt()\n",
    "    metrics, time = train_few_shot_convnext(model, train_dataloader, test_dataloader, epochs=10, lr=0.0001, optimizer='adam', scheduling=True)\n",
    "    metrics_list.append(metrics)"
   ],
   "id": "d69bd4687cc78184"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "experiment_name = \"FewShotConvNeXt with rotation augmentation\"\n",
    "multiple_runs_with_uncertainty_band(metrics_list, f\"Accuracy for {experiment_name}\", f\"Loss for {experiment_name}\")"
   ],
   "id": "f299d10ff6ff600",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data_dir = \"../../data\"\n",
    "dataset_name = \"augmented_cinic10_jitter\"\n",
    "train_dataloader = load_cinic10(data_dir, few_shot_per_class=100, dataset_name=dataset_name, split=\"none\")\n",
    "test_dataloader = load_cinic10(data_dir, few_shot_per_class=100, split=\"test\")"
   ],
   "id": "dc61df0c626c9f88",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "metrics_list = []\n",
    "for _ in range(5):\n",
    "    model = FewShotConvNeXt()\n",
    "    metrics, time = train_few_shot_convnext(model, train_dataloader, test_dataloader, epochs=10, lr=0.0001, optimizer='adam', scheduling=True)\n",
    "    metrics_list.append(metrics)"
   ],
   "id": "9dd4c63bda954d41"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "experiment_name = \"FewShotConvNeXt with jitter augmentation\"\n",
    "multiple_runs_with_uncertainty_band(metrics_list, f\"Accuracy for {experiment_name}\", f\"Loss for {experiment_name}\")"
   ],
   "id": "e21faf0b97401278",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data_dir = \"../../data\"\n",
    "dataset_name = \"augmented_cinic10_mixed\"\n",
    "train_dataloader = load_cinic10(data_dir, few_shot_per_class=100, dataset_name=dataset_name, split=\"none\")\n",
    "test_dataloader = load_cinic10(data_dir, few_shot_per_class=100, split=\"test\")"
   ],
   "id": "c03c309d155d38fe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "metrics_list = []\n",
    "for _ in range(5):\n",
    "    model = FewShotConvNeXt()\n",
    "    metrics, time = train_few_shot_convnext(model, train_dataloader, test_dataloader, epochs=10, lr=0.0001, optimizer='adam', scheduling=True)\n",
    "    metrics_list.append(metrics)"
   ],
   "id": "38f46b3d797ca889"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "experiment_name = \"FewShotConvNeXt with mixed augmentation\"\n",
    "multiple_runs_with_uncertainty_band(metrics_list, f\"Accuracy for {experiment_name}\", f\"Loss for {experiment_name}\")"
   ],
   "id": "1872ee1e61f249b4",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
