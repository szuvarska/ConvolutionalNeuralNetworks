{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal of this notebook is to improve previous attempts in a more structured way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "\n",
    "from helper_functions import train_step, test_step, accuracy_fn, print_train_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 213\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "cinic_directory = '../data'\n",
    "cinic_train = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.ImageFolder(cinic_directory + '/train',\n",
    "    \ttransform=transforms.ToTensor()),\n",
    "    batch_size=64, shuffle=True)\n",
    "\n",
    "cinic_test = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.ImageFolder(cinic_directory + '/test',\n",
    "    \ttransform=transforms.ToTensor()),\n",
    "    batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedModel(nn.Module):\n",
    "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Block 1: First set of convolutional layers\n",
    "        self.block_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input_shape, \n",
    "                      out_channels=hidden_units, \n",
    "                      kernel_size=3,\n",
    "                      stride=1,\n",
    "                      padding=1), \n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(hidden_units),  # Add batch normalization\n",
    "            nn.Conv2d(in_channels=hidden_units, \n",
    "                      out_channels=hidden_units,\n",
    "                      kernel_size=3,\n",
    "                      stride=1,\n",
    "                      padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(hidden_units),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)  # Reduce spatial dimensions\n",
    "        )\n",
    "        \n",
    "        # Block 2: Second set of convolutional layers\n",
    "        self.block_2 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_units, hidden_units*2, 3, padding=1),  # Increase filter size\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(hidden_units*2),\n",
    "            nn.Conv2d(hidden_units*2, hidden_units*2, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(hidden_units*2),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        \n",
    "        # Block 3: Third set of convolutional layers\n",
    "        self.block_3 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_units*2, hidden_units*4, 3, padding=1),  # Increase filter size again\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(hidden_units*4),\n",
    "            nn.Conv2d(hidden_units*4, hidden_units*4, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(hidden_units*4),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        \n",
    "        # Global Average Pooling to reduce the number of parameters\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        \n",
    "        # Fully connected layer (classifier)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.5),  # Add dropout to prevent overfitting\n",
    "            nn.Linear(in_features=hidden_units*4, out_features=output_shape)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.block_1(x)\n",
    "        x = self.block_2(x)\n",
    "        x = self.block_3(x)\n",
    "        x = self.global_pool(x)  # Global average pooling\n",
    "        x = self.classifier(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EnhancedModel(input_shape=3, \n",
    "    hidden_units=64, \n",
    "    output_shape=10).to(device)\n",
    "\n",
    "\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "---------\n",
      "Train loss: 1.51780 | Train accuracy: 44.27%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:22<03:26, 22.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1.33048 | Test accuracy: 52.33%\n",
      "\n",
      "Epoch: 1\n",
      "---------\n",
      "Train loss: 1.18208 | Train accuracy: 56.83%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:45<03:03, 22.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1.10877 | Test accuracy: 58.45%\n",
      "\n",
      "Epoch: 2\n",
      "---------\n",
      "Train loss: 0.99605 | Train accuracy: 64.07%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [01:08<02:40, 22.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1.03315 | Test accuracy: 62.85%\n",
      "\n",
      "Epoch: 3\n",
      "---------\n",
      "Train loss: 0.88425 | Train accuracy: 68.36%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [01:31<02:17, 22.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.99222 | Test accuracy: 64.39%\n",
      "\n",
      "Epoch: 4\n",
      "---------\n",
      "Train loss: 0.79697 | Train accuracy: 71.51%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [01:54<01:54, 22.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.91307 | Test accuracy: 67.54%\n",
      "\n",
      "Epoch: 5\n",
      "---------\n",
      "Train loss: 0.71317 | Train accuracy: 74.47%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [02:17<01:31, 22.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.88322 | Test accuracy: 70.08%\n",
      "\n",
      "Epoch: 6\n",
      "---------\n",
      "Train loss: 0.64298 | Train accuracy: 76.99%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [02:40<01:08, 22.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.87215 | Test accuracy: 70.27%\n",
      "\n",
      "Epoch: 7\n",
      "---------\n",
      "Train loss: 0.56530 | Train accuracy: 79.64%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [03:03<00:45, 22.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.95184 | Test accuracy: 69.93%\n",
      "\n",
      "Epoch: 8\n",
      "---------\n",
      "Train loss: 0.50247 | Train accuracy: 81.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [03:26<00:22, 22.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.98052 | Test accuracy: 69.89%\n",
      "\n",
      "Epoch: 9\n",
      "---------\n",
      "Train loss: 0.43555 | Train accuracy: 84.35%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [03:49<00:00, 22.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1.07100 | Test accuracy: 69.16%\n",
      "\n",
      "\n",
      "Train time on cuda: 229.287 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "train_time_start_model_2 = timer()\n",
    "\n",
    "# Train and test model \n",
    "epochs = 10\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    print(f\"Epoch: {epoch}\\n---------\")\n",
    "    train_step(data_loader=cinic_train, \n",
    "        model=model, \n",
    "        loss_fn=loss_fn,\n",
    "        optimizer=optimizer,\n",
    "        accuracy_fn=accuracy_fn,\n",
    "        device=device\n",
    "    )\n",
    "    test_step(data_loader=cinic_test,\n",
    "        model=model,\n",
    "        loss_fn=loss_fn,\n",
    "        accuracy_fn=accuracy_fn,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "train_time_end_model_2 = timer()\n",
    "total_train_time_model_2 = print_train_time(start=train_time_start_model_2,\n",
    "                                           end=train_time_end_model_2,\n",
    "                                           device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "EnhancedModel                            [64, 10]                  --\n",
       "├─Sequential: 1-1                        [64, 64, 16, 16]          --\n",
       "│    └─Conv2d: 2-1                       [64, 64, 32, 32]          1,792\n",
       "│    └─ReLU: 2-2                         [64, 64, 32, 32]          --\n",
       "│    └─BatchNorm2d: 2-3                  [64, 64, 32, 32]          128\n",
       "│    └─Conv2d: 2-4                       [64, 64, 32, 32]          36,928\n",
       "│    └─ReLU: 2-5                         [64, 64, 32, 32]          --\n",
       "│    └─BatchNorm2d: 2-6                  [64, 64, 32, 32]          128\n",
       "│    └─MaxPool2d: 2-7                    [64, 64, 16, 16]          --\n",
       "├─Sequential: 1-2                        [64, 128, 8, 8]           --\n",
       "│    └─Conv2d: 2-8                       [64, 128, 16, 16]         73,856\n",
       "│    └─ReLU: 2-9                         [64, 128, 16, 16]         --\n",
       "│    └─BatchNorm2d: 2-10                 [64, 128, 16, 16]         256\n",
       "│    └─Conv2d: 2-11                      [64, 128, 16, 16]         147,584\n",
       "│    └─ReLU: 2-12                        [64, 128, 16, 16]         --\n",
       "│    └─BatchNorm2d: 2-13                 [64, 128, 16, 16]         256\n",
       "│    └─MaxPool2d: 2-14                   [64, 128, 8, 8]           --\n",
       "├─Sequential: 1-3                        [64, 256, 4, 4]           --\n",
       "│    └─Conv2d: 2-15                      [64, 256, 8, 8]           295,168\n",
       "│    └─ReLU: 2-16                        [64, 256, 8, 8]           --\n",
       "│    └─BatchNorm2d: 2-17                 [64, 256, 8, 8]           512\n",
       "│    └─Conv2d: 2-18                      [64, 256, 8, 8]           590,080\n",
       "│    └─ReLU: 2-19                        [64, 256, 8, 8]           --\n",
       "│    └─BatchNorm2d: 2-20                 [64, 256, 8, 8]           512\n",
       "│    └─MaxPool2d: 2-21                   [64, 256, 4, 4]           --\n",
       "├─AdaptiveAvgPool2d: 1-4                 [64, 256, 1, 1]           --\n",
       "├─Sequential: 1-5                        [64, 10]                  --\n",
       "│    └─Flatten: 2-22                     [64, 256]                 --\n",
       "│    └─Dropout: 2-23                     [64, 256]                 --\n",
       "│    └─Linear: 2-24                      [64, 10]                  2,570\n",
       "==========================================================================================\n",
       "Total params: 1,149,770\n",
       "Trainable params: 1,149,770\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 9.79\n",
       "==========================================================================================\n",
       "Input size (MB): 0.79\n",
       "Forward/backward pass size (MB): 234.89\n",
       "Params size (MB): 4.60\n",
       "Estimated Total Size (MB): 240.27\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "model = EnhancedModel(input_shape=3, \n",
    "    hidden_units=64, \n",
    "    output_shape=10).to(device)\n",
    "\n",
    "summary(model, input_size=(64, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
