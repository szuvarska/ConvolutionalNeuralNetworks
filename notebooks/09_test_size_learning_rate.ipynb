{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "from baseline_model import BaselineModel\n",
    "from enhanced_model import EnhancedModel\n",
    "from helper_functions import run_model\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "data_loader_args = {\n",
    "    \"batch_size\": 64,\n",
    "    \"shuffle\": True,\n",
    "    \"num_workers\": 6,\n",
    "    \"pin_memory\": True,\n",
    "    \"persistent_workers\": True,\n",
    "}\n",
    "\n",
    "\n",
    "seed = 213\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try to capture Gpu utilization from btop during those test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/20 (25%) done, runs 0h:10min\n",
      "10/20 (50%) done, runs 0h:20min\n",
      "15/20 (75%) done, runs 0h:31min\n",
      "20/20 (100%) done, runs 0h:42min\n"
     ]
    }
   ],
   "source": [
    "# initial setup\n",
    "metadata = \"\"\"\"\"\"\n",
    "epochs = 30\n",
    "lr = 0.01\n",
    "num_of_repeats = 5\n",
    "dropout = 0.75\n",
    "regularization = 0.001\n",
    "hidden_units = [10, 20]  # [10, 20, 30, 40, 50, 60, 70, 80]\n",
    "learning_rates = [0.01, 0.001]  # [0.01, 0.001, 0.0001]\n",
    "experiment_dict = {\n",
    "    \"experiment name\": \"general_parameters\",\n",
    "    \"experiments metadata\": metadata,\n",
    "}\n",
    "if \"general_parameters.pkl\" not in os.listdir(\"../results\"):\n",
    "    pickle.dump(experiment_dict, open(\"../results/general_parameters.pkl\", \"wb\"))\n",
    "else:\n",
    "    raise FileExistsError(\n",
    "        \"File already exists, this notebook is not intended to run multiple times\"\n",
    "    )\n",
    "\n",
    "# time and progress\n",
    "start_time = time.time()\n",
    "total_number_of_runs = num_of_repeats * len(hidden_units) * len(learning_rates)\n",
    "number_of_completed_runs = 0\n",
    "\n",
    "cinic_directory = \"../data\"\n",
    "cinic_train = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.ImageFolder(\n",
    "        cinic_directory + \"/train\", transform=transforms.ToTensor()\n",
    "    ),\n",
    "    **data_loader_args,\n",
    ")\n",
    "\n",
    "cinic_test = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.ImageFolder(\n",
    "        cinic_directory + \"/test\", transform=transforms.ToTensor()\n",
    "    ),\n",
    "    **data_loader_args,\n",
    ")\n",
    "\n",
    "for hidden_unit in hidden_units:\n",
    "    for lr in learning_rates:\n",
    "        experiment_dict = pickle.load(open(\"../results/general_parameters.pkl\", \"rb\"))\n",
    "        metrics_list = []\n",
    "        total_time_list = []\n",
    "        for i in range(num_of_repeats):\n",
    "            model = BaselineModel(\n",
    "                input_shape=3,\n",
    "                hidden_units=hidden_unit,\n",
    "                output_shape=10,\n",
    "                dropout_p=dropout,\n",
    "            ).to(device)\n",
    "\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            optimizer = torch.optim.SGD(\n",
    "                params=model.parameters(),\n",
    "                lr=lr,\n",
    "                momentum=0.9,\n",
    "                weight_decay=regularization,\n",
    "            )\n",
    "\n",
    "            metrics, total_time = run_model(\n",
    "                cinic_train,\n",
    "                cinic_test,\n",
    "                model,\n",
    "                loss_fn,\n",
    "                optimizer,\n",
    "                device=device,\n",
    "                epochs=epochs,\n",
    "            )\n",
    "\n",
    "            metrics_list.append(metrics)\n",
    "            total_time_list.append(total_time)\n",
    "            number_of_completed_runs += 1\n",
    "\n",
    "        # add info about number of parameters\n",
    "        total_number_params = sum(p.numel() for p in model.parameters())\n",
    "        experiment_dict[(hidden_unit, lr)] = (\n",
    "            metrics_list,  # accuracy and loss for each epoch\n",
    "            total_time_list,  # time it took to train\n",
    "            total_number_params,  # number of parameters\n",
    "        )\n",
    "        pickle.dump(experiment_dict, open(\"../results/general_parameters.pkl\", \"wb\"))\n",
    "        seconds = time.time() - start_time\n",
    "        hours = int(seconds // 3600)\n",
    "        minutes = int((seconds % 3600) // 60)\n",
    "        print(\n",
    "            f\"{number_of_completed_runs}/{total_number_of_runs} ({round(100* number_of_completed_runs/total_number_of_runs)}%) done, runs {hours}h:{minutes}min\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial setup\n",
    "metadata = \"\"\"\"\"\"\n",
    "epochs = 30\n",
    "lr = 0.01\n",
    "num_of_repeats = 3\n",
    "dropout = 0.75\n",
    "regularization = 0.001\n",
    "hidden_units = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "learning_rates = [0.01]\n",
    "experiment_dict = {\n",
    "    \"experiment name\": \"enhanced_model_time\",\n",
    "    \"experiments metadata\": metadata,\n",
    "}\n",
    "if \"enhanced_model_time.pkl\" not in os.listdir(\"../results\"):\n",
    "    pickle.dump(experiment_dict, open(\"../results/enhanced_model_time.pkl\", \"wb\"))\n",
    "else:\n",
    "    raise FileExistsError(\n",
    "        \"File already exists, this notebook is not intended to run multiple times\"\n",
    "    )\n",
    "\n",
    "# time and progress\n",
    "start_time = time.time()\n",
    "total_number_of_runs = num_of_repeats * len(hidden_units) * len(learning_rates)\n",
    "number_of_completed_runs = 0\n",
    "\n",
    "cinic_directory = \"../data\"\n",
    "cinic_train = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.ImageFolder(\n",
    "        cinic_directory + \"/train\", transform=transforms.ToTensor()\n",
    "    ),\n",
    "    **data_loader_args,\n",
    ")\n",
    "\n",
    "cinic_test = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.ImageFolder(\n",
    "        cinic_directory + \"/test\", transform=transforms.ToTensor()\n",
    "    ),\n",
    "    **data_loader_args,\n",
    ")\n",
    "\n",
    "for hidden_unit in hidden_units:\n",
    "    for lr in learning_rates:\n",
    "        experiment_dict = pickle.load(open(\"../results/enhanced_model_time.pkl\", \"rb\"))\n",
    "        metrics_list = []\n",
    "        total_time_list = []\n",
    "        for i in range(num_of_repeats):\n",
    "            model = EnhancedModel(\n",
    "                input_shape=3,\n",
    "                hidden_units=hidden_unit,\n",
    "                output_shape=10,\n",
    "                dropout_p=dropout,\n",
    "            ).to(device)\n",
    "\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            optimizer = torch.optim.SGD(\n",
    "                params=model.parameters(),\n",
    "                lr=lr,\n",
    "                momentum=0.9,\n",
    "                weight_decay=regularization,\n",
    "            )\n",
    "\n",
    "            metrics, total_time = run_model(\n",
    "                cinic_train,\n",
    "                cinic_test,\n",
    "                model,\n",
    "                loss_fn,\n",
    "                optimizer,\n",
    "                device=device,\n",
    "                epochs=epochs,\n",
    "            )\n",
    "\n",
    "            metrics_list.append(metrics)\n",
    "            total_time_list.append(total_time)\n",
    "            number_of_completed_runs += 1\n",
    "\n",
    "        # add info about number of parameters\n",
    "        total_number_params = sum(p.numel() for p in model.parameters())\n",
    "        experiment_dict[(hidden_unit, lr)] = (\n",
    "            metrics_list,  # accuracy and loss for each epoch\n",
    "            total_time_list,  # time it took to train\n",
    "            total_number_params,  # number of parameters\n",
    "        )\n",
    "        pickle.dump(experiment_dict, open(\"../results/enhanced_model_time.pkl\", \"wb\"))\n",
    "        seconds = time.time() - start_time\n",
    "        hours = int(seconds // 3600)\n",
    "        minutes = int((seconds % 3600) // 60)\n",
    "        print(\n",
    "            f\"{number_of_completed_runs}/{total_number_of_runs} ({round(100* number_of_completed_runs/total_number_of_runs)}%) done, runs {hours}h:{minutes}min\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
