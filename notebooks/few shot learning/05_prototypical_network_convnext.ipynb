{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-17T20:58:41.499871Z",
     "start_time": "2025-03-17T20:58:33.667994Z"
    }
   },
   "source": [
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from src.few_shot_learning import load_cinic10, calculate_accuracy, plot_confusion_matrix\n",
    "from torch.optim.lr_scheduler import StepLR"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T20:58:41.503937Z",
     "start_time": "2025-03-17T20:58:41.501Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ProtoConvNeXt(nn.Module):\n",
    "    def __init__(self, feature_dim=128):\n",
    "        super(ProtoConvNeXt, self).__init__()\n",
    "        self.backbone = timm.create_model('convnext_tiny', pretrained=True)  # Load ConvNeXt\n",
    "        self.backbone.head.fc = nn.Identity()  # Remove classification layer\n",
    "        self.projection = nn.Linear(768, feature_dim)  # Map to embedding space\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)  # Extract features\n",
    "        return self.projection(features)  # Reduce to feature_dim"
   ],
   "id": "a4afcb9ce56374f3",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T21:22:41.666190Z",
     "start_time": "2025-03-17T21:22:41.662206Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def prototypical_loss(support_embeddings, support_labels, query_embeddings, query_labels, num_classes):\n",
    "\n",
    "    support_embeddings = F.normalize(support_embeddings, p=2, dim=1)  # L2 Normalisation\n",
    "    query_embeddings = F.normalize(query_embeddings, p=2, dim=1)\n",
    "\n",
    "    prototypes = []\n",
    "    for cls in range(num_classes):\n",
    "        cls_mask = (support_labels == cls)\n",
    "        \n",
    "        if cls_mask.sum() == 0:  # Avoid empty classes\n",
    "            prototypes.append(torch.zeros((1, support_embeddings.shape[1]), device=support_embeddings.device))\n",
    "        else:\n",
    "            cls_proto = support_embeddings[cls_mask].mean(dim=0, keepdim=True)  # Keep [1, feature_dim]\n",
    "            prototypes.append(cls_proto)\n",
    "\n",
    "    prototypes = torch.cat(prototypes, dim=0)  # Stack into (num_classes, feature_dim)\n",
    "\n",
    "    # Compute squared Euclidean distance\n",
    "    dists = torch.cdist(query_embeddings, prototypes, p=2)\n",
    "\n",
    "    # Convert distances to probabilities\n",
    "    log_p_y = F.log_softmax(-dists, dim=1)  # Negative distance as similarity\n",
    "    loss = F.nll_loss(log_p_y, query_labels)\n",
    "\n",
    "    preds = log_p_y.argmax(dim=1)\n",
    "    acc = (preds == query_labels).float().mean().item()\n",
    "\n",
    "    return loss, acc\n"
   ],
   "id": "5b855a48293d22a4",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T21:22:43.626603Z",
     "start_time": "2025-03-17T21:22:43.622118Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_prototypical(model, dataloader, epochs=10, lr=0.0001, num_classes=10, N_shot=5, N_query=5):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    scheduler = StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss, total_acc = 0, 0\n",
    "\n",
    "        for images, labels in dataloader:  # Assume balanced batch per class\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Randomly sample support and query sets\n",
    "            indices = torch.randperm(images.size(0))\n",
    "            support_idx, query_idx = indices[:N_shot*num_classes], indices[N_shot*num_classes:]\n",
    "            \n",
    "            support_images, support_labels = images[support_idx], labels[support_idx]\n",
    "            query_images, query_labels = images[query_idx], labels[query_idx]\n",
    "\n",
    "            # Forward pass\n",
    "            support_embeddings = model(support_images)\n",
    "            query_embeddings = model(query_images)\n",
    "\n",
    "            # Compute prototypical loss\n",
    "            loss, acc = prototypical_loss(support_embeddings, support_labels, query_embeddings, query_labels, num_classes)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_acc += acc\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {total_loss:.4f}, Acc: {total_acc/len(dataloader):.4f}\")\n"
   ],
   "id": "c36f6315eaa6c13",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T21:24:25.116156Z",
     "start_time": "2025-03-17T21:22:49.559231Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_dir = \"../../data\"\n",
    "dataloader = load_cinic10(data_dir, few_shot_per_class=100, batch_size=128)\n",
    "model = ProtoConvNeXt()\n",
    "train_prototypical(model, dataloader, epochs=30, lr=0.0001, num_classes=10, N_shot=10, N_query=5)"
   ],
   "id": "7dbb47fae600faf7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Loss: 16.2234, Acc: 0.3750\n",
      "Epoch [2/30], Loss: 14.6931, Acc: 0.3393\n",
      "Epoch [3/30], Loss: 14.8907, Acc: 0.4375\n",
      "Epoch [4/30], Loss: 13.9202, Acc: 0.4330\n",
      "Epoch [5/30], Loss: 13.5166, Acc: 0.5268\n",
      "Epoch [6/30], Loss: 12.9577, Acc: 0.5804\n",
      "Epoch [7/30], Loss: 12.6123, Acc: 0.5625\n",
      "Epoch [8/30], Loss: 12.2977, Acc: 0.6563\n",
      "Epoch [9/30], Loss: 12.3473, Acc: 0.6250\n",
      "Epoch [10/30], Loss: 12.0317, Acc: 0.6786\n",
      "Epoch [11/30], Loss: 11.9193, Acc: 0.7277\n",
      "Epoch [12/30], Loss: 11.7431, Acc: 0.6741\n",
      "Epoch [13/30], Loss: 11.6248, Acc: 0.7009\n",
      "Epoch [14/30], Loss: 11.2430, Acc: 0.7500\n",
      "Epoch [15/30], Loss: 11.4542, Acc: 0.7902\n",
      "Epoch [16/30], Loss: 11.3084, Acc: 0.7768\n",
      "Epoch [17/30], Loss: 11.2786, Acc: 0.8170\n",
      "Epoch [18/30], Loss: 10.8633, Acc: 0.8661\n",
      "Epoch [19/30], Loss: 10.8716, Acc: 0.8571\n",
      "Epoch [20/30], Loss: 10.9152, Acc: 0.8304\n",
      "Epoch [21/30], Loss: 10.8023, Acc: 0.8482\n",
      "Epoch [22/30], Loss: 10.7338, Acc: 0.8750\n",
      "Epoch [23/30], Loss: 10.8194, Acc: 0.8616\n",
      "Epoch [24/30], Loss: 10.5974, Acc: 0.9330\n",
      "Epoch [25/30], Loss: 10.9075, Acc: 0.8616\n",
      "Epoch [26/30], Loss: 10.5538, Acc: 0.9063\n",
      "Epoch [27/30], Loss: 10.7334, Acc: 0.8795\n",
      "Epoch [28/30], Loss: 10.5769, Acc: 0.9241\n",
      "Epoch [29/30], Loss: 10.7344, Acc: 0.8750\n",
      "Epoch [30/30], Loss: 10.6106, Acc: 0.9375\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T21:12:06.227557Z",
     "start_time": "2025-03-17T21:12:06.221894Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_prototypical_accuracy(model, data_root, split='test', batch_size=32, num_classes=10, N_shot=5):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    test_loader = load_cinic10(data_root, split=split, few_shot_per_class=1000, batch_size=batch_size)\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Randomly split into support & query\n",
    "            indices = torch.randperm(images.size(0))\n",
    "            support_idx, query_idx = indices[:N_shot*num_classes], indices[N_shot*num_classes:]\n",
    "\n",
    "            support_images, support_labels = images[support_idx], labels[support_idx]\n",
    "            query_images, query_labels = images[query_idx], labels[query_idx]\n",
    "\n",
    "            # Get embeddings\n",
    "            support_embeddings = model(support_images)\n",
    "            query_embeddings = model(query_images)\n",
    "\n",
    "            # Compute prototypes\n",
    "            prototypes = []\n",
    "            for c in range(num_classes):\n",
    "                class_mask = support_labels == c\n",
    "                if class_mask.sum() > 0:  # Avoid empty classes\n",
    "                    class_prototype = support_embeddings[class_mask].mean(0)\n",
    "                    prototypes.append(class_prototype)\n",
    "\n",
    "            prototypes = torch.stack(prototypes)  # Shape: [num_classes, embedding_dim]\n",
    "\n",
    "            # Compute distances\n",
    "            distances = torch.cdist(query_embeddings, prototypes)  # Shape: [num_query, num_classes]\n",
    "            predicted_labels = torch.argmin(distances, dim=1)  # Nearest prototype = predicted class\n",
    "\n",
    "            correct += (predicted_labels == query_labels).sum().item()\n",
    "            total += query_labels.size(0)\n",
    "\n",
    "    accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "    print(f\"Accuracy on {split} set: {accuracy:.2f}%\")\n",
    "    return accuracy\n"
   ],
   "id": "ee4bbb4af4b72e09",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T21:28:27.953600Z",
     "start_time": "2025-03-17T21:28:20.355531Z"
    }
   },
   "cell_type": "code",
   "source": "calculate_prototypical_accuracy(model, data_dir, split='train', batch_size=128, num_classes=10, N_shot=10)",
   "id": "70ef7321af9ac2d7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on train set: 47.30%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "47.298534798534796"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T21:28:44.522143Z",
     "start_time": "2025-03-17T21:28:36.766307Z"
    }
   },
   "cell_type": "code",
   "source": "calculate_prototypical_accuracy(model, data_dir, split='test', batch_size=128, num_classes=10, N_shot=10)",
   "id": "9ae4cd1104d1e13",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 45.51%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "45.51282051282051"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "cfb3f86bf3e49dc8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
