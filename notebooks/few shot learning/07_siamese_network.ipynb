{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-17T22:27:02.941810Z",
     "start_time": "2025-03-17T22:26:54.202518Z"
    }
   },
   "source": [
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from src.few_shot_learning import load_cinic10, calculate_accuracy, plot_confusion_matrix\n",
    "from torch.optim.lr_scheduler import StepLR"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T22:27:02.945896Z",
     "start_time": "2025-03-17T22:27:02.942742Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the basic architecture for the subnetwork (the same one used for both inputs)\n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.fc = nn.Linear(128 * 32 * 32, 128)  # Final embedding dimension\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        embedding = self.fc(x)\n",
    "        return embedding"
   ],
   "id": "3a0e50a935a58a11",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T22:27:02.949305Z",
     "start_time": "2025-03-17T22:27:02.946551Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Contrastive loss function\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2, p=2)\n",
    "        loss = torch.mean((1 - label) * torch.pow(euclidean_distance, 2) + \n",
    "                          (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
    "        return loss"
   ],
   "id": "59cd21bf36d6e51e",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T22:27:02.953904Z",
     "start_time": "2025-03-17T22:27:02.950562Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_siamese_network(model, dataloader, epochs=10, lr=0.0001):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    criterion = ContrastiveLoss(margin=1.0)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for data1, data2, labels in dataloader:\n",
    "            data1, data2, labels = data1.to(device), data2.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            output1 = model(data1)\n",
    "            output2 = model(data2)\n",
    "            \n",
    "            # Calculate contrastive loss\n",
    "            loss = criterion(output1, output2, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {total_loss:.4f}\")"
   ],
   "id": "9a8854fb7a08cc4",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T22:27:02.961331Z",
     "start_time": "2025-03-17T22:27:02.954619Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class FewShotSiameseDataset(Dataset):\n",
    "    def __init__(self, dataset, n_shot, n_query, n_classes, transform=None):\n",
    "        \"\"\"\n",
    "        Initialize the FewShotSiameseDataset.\n",
    "\n",
    "        dataset (torchvision dataset): Dataset to sample from (e.g., CIFAR-10)\n",
    "        n_shot (int): Number of support samples per class (5)\n",
    "        n_query (int): Number of query samples per class (5)\n",
    "        n_classes (int): Number of classes in the few-shot episode\n",
    "        transform (callable): Optional transform to be applied on an image.\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.n_shot = n_shot\n",
    "        self.n_query = n_query\n",
    "        self.n_classes = n_classes\n",
    "        self.transform = transform\n",
    "\n",
    "        # Create class-to-images mapping\n",
    "        self.class_to_images = {}\n",
    "        for img, label in dataset:\n",
    "            if label not in self.class_to_images:\n",
    "                self.class_to_images[label] = []\n",
    "            self.class_to_images[label].append(img)\n",
    "\n",
    "        # Limit the dataset to only `n_classes`\n",
    "        self.selected_classes = random.sample(list(self.class_to_images.keys()), n_classes)\n",
    "\n",
    "        # Ensure each class has at least 10 images (adjusting for few-shot)\n",
    "        for class_idx in self.selected_classes:\n",
    "            assert len(self.class_to_images[class_idx]) >= 10, f\"Class {class_idx} has less than 10 images.\"\n",
    "\n",
    "    def __len__(self):\n",
    "        # Number of few-shot episodes (here we generate 1000 episodes)\n",
    "        return 1000\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Randomly sample `n_classes` and then `n_shot` support images and `n_query` query images\n",
    "        selected_classes = random.sample(self.selected_classes, self.n_classes)\n",
    "\n",
    "        support_images, support_labels = [], []\n",
    "        query_images, query_labels = [], []\n",
    "\n",
    "        # Get support and query images for each class\n",
    "        for class_idx in selected_classes:\n",
    "            images = self.class_to_images[class_idx]\n",
    "            random.shuffle(images)\n",
    "\n",
    "            # Select 10 images for each class\n",
    "            support_images.extend(images[:self.n_shot])\n",
    "            support_labels.extend([class_idx] * self.n_shot)\n",
    "            query_images.extend(images[self.n_shot:self.n_shot + self.n_query])\n",
    "            query_labels.extend([class_idx] * self.n_query)\n",
    "\n",
    "        # Create pairs (support, query) and label them (1 for same class, 0 for different class)\n",
    "        image_pairs, labels = [], []\n",
    "\n",
    "        # Create positive pairs (same class)\n",
    "        for i in range(len(support_images)):\n",
    "            for j in range(len(query_images)):\n",
    "                if support_labels[i] == query_labels[j]:\n",
    "                    image_pairs.append((support_images[i], query_images[j]))\n",
    "                    labels.append(1)  # Same class\n",
    "        \n",
    "        # Create negative pairs (different classes)\n",
    "        for i in range(len(support_images)):\n",
    "            for j in range(len(query_images)):\n",
    "                if support_labels[i] != query_labels[j]:\n",
    "                    image_pairs.append((support_images[i], query_images[j]))\n",
    "                    labels.append(0)  # Different class\n",
    "\n",
    "        # Apply transformations if defined\n",
    "        if self.transform:\n",
    "            image_pairs = [(self.transform(img1), self.transform(img2)) for img1, img2 in image_pairs]\n",
    "\n",
    "        return image_pairs, torch.tensor(labels)\n"
   ],
   "id": "bbcae19911c05b0c",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T22:31:12.210087Z",
     "start_time": "2025-03-17T22:27:02.962187Z"
    }
   },
   "cell_type": "code",
   "source": [
    "transform = transforms.Compose([transforms.Resize((64, 64)), transforms.ToTensor()])\n",
    "cifar10 = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "# Define the few-shot dataset\n",
    "few_shot_dataset = FewShotSiameseDataset(cifar10, n_shot=5, n_query=5, n_classes=5, transform=transform)\n",
    "\n",
    "# DataLoader to feed the model\n",
    "siamese_dataloader = torch.utils.data.DataLoader(few_shot_dataset, batch_size=32, shuffle=True)"
   ],
   "id": "27797e202aa84b49",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170M/170M [03:55<00:00, 725kB/s] \n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T22:31:14.252936Z",
     "start_time": "2025-03-17T22:31:12.210985Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = SiameseNetwork()\n",
    "train_siamese_network(model, siamese_dataloader, epochs=10)"
   ],
   "id": "2b5d43e8c635ae52",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "pic should be PIL Image or ndarray. Got <class 'torch.Tensor'>",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m model \u001B[38;5;241m=\u001B[39m SiameseNetwork()\n\u001B[0;32m----> 2\u001B[0m \u001B[43mtrain_siamese_network\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msiamese_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[4], line 10\u001B[0m, in \u001B[0;36mtrain_siamese_network\u001B[0;34m(model, dataloader, epochs, lr)\u001B[0m\n\u001B[1;32m      8\u001B[0m model\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[1;32m      9\u001B[0m total_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.0\u001B[39m\n\u001B[0;32m---> 10\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m data1, data2, labels \u001B[38;5;129;01min\u001B[39;00m dataloader:\n\u001B[1;32m     11\u001B[0m     data1, data2, labels \u001B[38;5;241m=\u001B[39m data1\u001B[38;5;241m.\u001B[39mto(device), data2\u001B[38;5;241m.\u001B[39mto(device), labels\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m     13\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n",
      "File \u001B[0;32m/media/marta/Dane/Sem8/ConvolutionalNeuralNeutworks/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:708\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    705\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    706\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    707\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 708\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    709\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    710\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m    711\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable\n\u001B[1;32m    712\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    713\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called\n\u001B[1;32m    714\u001B[0m ):\n",
      "File \u001B[0;32m/media/marta/Dane/Sem8/ConvolutionalNeuralNeutworks/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:764\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    762\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    763\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m--> 764\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m    765\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[1;32m    766\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[0;32m/media/marta/Dane/Sem8/ConvolutionalNeuralNeutworks/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     50\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 52\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[1;32m     53\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     54\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[0;32m/media/marta/Dane/Sem8/ConvolutionalNeuralNeutworks/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     50\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 52\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[1;32m     53\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     54\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "Cell \u001B[0;32mIn[5], line 77\u001B[0m, in \u001B[0;36mFewShotSiameseDataset.__getitem__\u001B[0;34m(self, index)\u001B[0m\n\u001B[1;32m     75\u001B[0m \u001B[38;5;66;03m# Apply transformations if defined\u001B[39;00m\n\u001B[1;32m     76\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform:\n\u001B[0;32m---> 77\u001B[0m     image_pairs \u001B[38;5;241m=\u001B[39m [(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform(img1), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform(img2)) \u001B[38;5;28;01mfor\u001B[39;00m img1, img2 \u001B[38;5;129;01min\u001B[39;00m image_pairs]\n\u001B[1;32m     79\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m image_pairs, torch\u001B[38;5;241m.\u001B[39mtensor(labels)\n",
      "Cell \u001B[0;32mIn[5], line 77\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     75\u001B[0m \u001B[38;5;66;03m# Apply transformations if defined\u001B[39;00m\n\u001B[1;32m     76\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform:\n\u001B[0;32m---> 77\u001B[0m     image_pairs \u001B[38;5;241m=\u001B[39m [(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg1\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform(img2)) \u001B[38;5;28;01mfor\u001B[39;00m img1, img2 \u001B[38;5;129;01min\u001B[39;00m image_pairs]\n\u001B[1;32m     79\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m image_pairs, torch\u001B[38;5;241m.\u001B[39mtensor(labels)\n",
      "File \u001B[0;32m/media/marta/Dane/Sem8/ConvolutionalNeuralNeutworks/.venv/lib/python3.10/site-packages/torchvision/transforms/transforms.py:95\u001B[0m, in \u001B[0;36mCompose.__call__\u001B[0;34m(self, img)\u001B[0m\n\u001B[1;32m     93\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, img):\n\u001B[1;32m     94\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransforms:\n\u001B[0;32m---> 95\u001B[0m         img \u001B[38;5;241m=\u001B[39m \u001B[43mt\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     96\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m img\n",
      "File \u001B[0;32m/media/marta/Dane/Sem8/ConvolutionalNeuralNeutworks/.venv/lib/python3.10/site-packages/torchvision/transforms/transforms.py:137\u001B[0m, in \u001B[0;36mToTensor.__call__\u001B[0;34m(self, pic)\u001B[0m\n\u001B[1;32m    129\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, pic):\n\u001B[1;32m    130\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    131\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[1;32m    132\u001B[0m \u001B[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    135\u001B[0m \u001B[38;5;124;03m        Tensor: Converted image.\u001B[39;00m\n\u001B[1;32m    136\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 137\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto_tensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpic\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/media/marta/Dane/Sem8/ConvolutionalNeuralNeutworks/.venv/lib/python3.10/site-packages/torchvision/transforms/functional.py:142\u001B[0m, in \u001B[0;36mto_tensor\u001B[0;34m(pic)\u001B[0m\n\u001B[1;32m    140\u001B[0m     _log_api_usage_once(to_tensor)\n\u001B[1;32m    141\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (F_pil\u001B[38;5;241m.\u001B[39m_is_pil_image(pic) \u001B[38;5;129;01mor\u001B[39;00m _is_numpy(pic)):\n\u001B[0;32m--> 142\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpic should be PIL Image or ndarray. Got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(pic)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    144\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _is_numpy(pic) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m _is_numpy_image(pic):\n\u001B[1;32m    145\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpic should be 2/3 dimensional. Got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpic\u001B[38;5;241m.\u001B[39mndim\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m dimensions.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mTypeError\u001B[0m: pic should be PIL Image or ndarray. Got <class 'torch.Tensor'>"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "b24a922c6d244d7a",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
