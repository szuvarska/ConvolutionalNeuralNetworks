{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-29T15:34:11.117521Z",
     "start_time": "2025-03-29T15:34:11.114917Z"
    }
   },
   "source": [
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from src.few_shot_learning import load_cinic10, calculate_accuracy, plot_confusion_matrix\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import random\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset\n",
    "import os"
   ],
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T15:20:25.854307Z",
     "start_time": "2025-03-29T15:20:25.850390Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the basic architecture for the subnetwork (the same one used for both inputs)\n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=5, stride=1, padding=2)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=5, stride=1, padding=2)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Dummy forward pass to determine fc input size\n",
    "        dummy_input = torch.randn(1, 3, 64, 64)  # Assuming input size 64x64\n",
    "        dummy_output = self.pool(F.relu(self.conv2(self.pool(F.relu(self.conv1(dummy_input))))))\n",
    "        flattened_size = dummy_output.view(1, -1).shape[1]  # Get correct size\n",
    "    \n",
    "        self.fc = nn.Linear(flattened_size, 128)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        return self.fc(x)\n"
   ],
   "id": "3a0e50a935a58a11",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T15:09:06.700141Z",
     "start_time": "2025-03-29T15:09:06.697205Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Contrastive loss function\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2, p=2)\n",
    "        loss = torch.mean((1 - label) * torch.pow(euclidean_distance, 2) + \n",
    "                          (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
    "        return loss"
   ],
   "id": "59cd21bf36d6e51e",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T15:34:35.196022Z",
     "start_time": "2025-03-29T15:34:35.192235Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_siamese_network(model, dataloader, epochs=10, lr=0.0001):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    criterion = ContrastiveLoss(margin=1.0)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for data1, data2, labels in dataloader:\n",
    "            data1, data2, labels = data1.to(device), data2.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            output1 = model(data1)\n",
    "            output2 = model(data2)\n",
    "            \n",
    "            # Calculate contrastive loss\n",
    "            loss = criterion(output1, output2, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {total_loss:.4f}\")"
   ],
   "id": "9a8854fb7a08cc4",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T15:33:23.016438Z",
     "start_time": "2025-03-29T15:33:23.009607Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CINIC10SiameseDataset(Dataset):\n",
    "    def __init__(self, root_dir, num_samples_per_class=100, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.num_samples_per_class = num_samples_per_class\n",
    "        \n",
    "        # Get all class names\n",
    "        self.classes = os.listdir(root_dir)\n",
    "        \n",
    "        # Limit the number of images per class to `num_samples_per_class`\n",
    "        self.image_paths = {}\n",
    "        for c in self.classes:\n",
    "            all_images = os.listdir(os.path.join(root_dir, c))\n",
    "            sampled_images = random.sample(all_images, min(len(all_images), num_samples_per_class))\n",
    "            self.image_paths[c] = [os.path.join(root_dir, c, img) for img in sampled_images]\n",
    "\n",
    "    def __len__(self):\n",
    "        return sum(len(imgs) for imgs in self.image_paths.values())\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Randomly select a class\n",
    "        class_name = random.choice(self.classes)\n",
    "        img1_path = random.choice(self.image_paths[class_name])\n",
    "\n",
    "        # Decide if this will be a positive or negative pair\n",
    "        if random.random() > 0.5:  # 50% chance of being a positive pair\n",
    "            img2_path = random.choice(self.image_paths[class_name])\n",
    "            label = 1\n",
    "        else:  # Negative pair\n",
    "            different_class = random.choice([c for c in self.classes if c != class_name])\n",
    "            img2_path = random.choice(self.image_paths[different_class])\n",
    "            label = 0\n",
    "\n",
    "        # Load images\n",
    "        img1 = Image.open(img1_path).convert(\"RGB\")\n",
    "        img2 = Image.open(img2_path).convert(\"RGB\")\n",
    "\n",
    "        # Apply transformations\n",
    "        if self.transform:\n",
    "            img1 = self.transform(img1)\n",
    "            img2 = self.transform(img2)\n",
    "\n",
    "        return img1, img2, torch.tensor(label, dtype=torch.float32)\n"
   ],
   "id": "bbcae19911c05b0c",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T15:33:40.437088Z",
     "start_time": "2025-03-29T15:33:40.348934Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define transformations (remove ToTensor from dataset loading)\n",
    "transform = transforms.Compose([transforms.Resize((64, 64)), transforms.ToTensor()])\n",
    "\n",
    "dataset_path = '../../data/cinic-10/train'\n",
    "\n",
    "# Define the few-shot dataset\n",
    "siamese_dataset = CINIC10SiameseDataset(root_dir=dataset_path, transform=transform)\n",
    "\n",
    "# DataLoader to feed the model\n",
    "siamese_dataloader = torch.utils.data.DataLoader(siamese_dataset, batch_size=32, shuffle=True)"
   ],
   "id": "27797e202aa84b49",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T15:38:31.759147Z",
     "start_time": "2025-03-29T15:38:31.755348Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "def evaluate_siamese_network(model, dataloader, threshold=0.5):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # No gradients needed for evaluation\n",
    "        for img1, img2, labels in dataloader:\n",
    "            img1, img2, labels = img1.to(device), img2.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            output1 = model(img1)\n",
    "            output2 = model(img2)\n",
    "\n",
    "            # Compute Euclidean distance (or L1 distance)\n",
    "            distances = torch.norm(output1 - output2, p=2, dim=1)\n",
    "\n",
    "            # Convert distances to binary predictions\n",
    "            predictions = (distances < threshold).float()\n",
    "\n",
    "            # Count correct predictions\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    accuracy = (correct / total) * 100\n",
    "    print(f'Accuracy: {accuracy:.2f}%')\n",
    "    return accuracy\n"
   ],
   "id": "207976a4a734c0a9",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T15:38:01.313942Z",
     "start_time": "2025-03-29T15:36:44.485317Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = SiameseNetwork()\n",
    "train_siamese_network(model, siamese_dataloader, epochs=30)"
   ],
   "id": "2b5d43e8c635ae52",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Loss: 9.3954\n",
      "Epoch [2/30], Loss: 9.1318\n",
      "Epoch [3/30], Loss: 8.9913\n",
      "Epoch [4/30], Loss: 8.7910\n",
      "Epoch [5/30], Loss: 8.9604\n",
      "Epoch [6/30], Loss: 8.6815\n",
      "Epoch [7/30], Loss: 8.8289\n",
      "Epoch [8/30], Loss: 8.4779\n",
      "Epoch [9/30], Loss: 8.8322\n",
      "Epoch [10/30], Loss: 8.6088\n",
      "Epoch [11/30], Loss: 8.5445\n",
      "Epoch [12/30], Loss: 8.5265\n",
      "Epoch [13/30], Loss: 8.7521\n",
      "Epoch [14/30], Loss: 8.7096\n",
      "Epoch [15/30], Loss: 8.5977\n",
      "Epoch [16/30], Loss: 8.4794\n",
      "Epoch [17/30], Loss: 8.4841\n",
      "Epoch [18/30], Loss: 8.6347\n",
      "Epoch [19/30], Loss: 8.3687\n",
      "Epoch [20/30], Loss: 8.5102\n",
      "Epoch [21/30], Loss: 8.6155\n",
      "Epoch [22/30], Loss: 8.5831\n",
      "Epoch [23/30], Loss: 8.5597\n",
      "Epoch [24/30], Loss: 8.6220\n",
      "Epoch [25/30], Loss: 8.6080\n",
      "Epoch [26/30], Loss: 8.4588\n",
      "Epoch [27/30], Loss: 8.6239\n",
      "Epoch [28/30], Loss: 8.5991\n",
      "Epoch [29/30], Loss: 8.7450\n",
      "Epoch [30/30], Loss: 8.4158\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T15:38:40.766281Z",
     "start_time": "2025-03-29T15:38:39.308665Z"
    }
   },
   "cell_type": "code",
   "source": "train_accuracy = evaluate_siamese_network(model, siamese_dataloader)",
   "id": "b24a922c6d244d7a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 53.00%\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1888d0bf0bf3980"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
